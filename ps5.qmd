---
title: "PS5 Andy Fan Will Sigal"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 9 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)

## Submission Steps (10 pts)

1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):  Andy Fan, fanx
    - Partner 2 (name and cnet ID):  Will Sigal, Wsigal
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: 

AF    WS

5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: (Andy:0); (Will:0) Late coins left after submission: (Andy: 3) ; (Will: 4)
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope


```{python}
### SETUP 
import pandas as pd
import altair as alt
import time
import os
import warnings
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
warnings.filterwarnings('ignore')
import requests
from bs4 import BeautifulSoup
import concurrent.futures

```


## (30 points) Step 1: Develop initial scraper and crawler

#### 1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”page and scrape and collect the following into a dataset: • Title of the enforcement action • Date • Category (e.g, “Criminal and Civil Actions”) • Link associated with the enforcement action Collect your output into a tidy dataframe and print its head.

```{python}
### making soup
url1 = 'https://oig.hhs.gov/fraud/enforcement'
response1 = requests.get(url1)
soup1 = BeautifulSoup(response1.content, 'lxml')
```

```{python}
### find title of enforcements
li_blocks = soup1.find_all('h2') #h2 classes with nested 'a' titles. li_blocks[2:21] are the 20 ones
li_titles = []
for h2 in li_blocks:
    # Find all 'a' tags within each 'h2' element
    for a_tag in h2.find_all('a'):
        li_titles.append(a_tag)
li_titles[0:5]
df_title = pd.DataFrame(li_titles) # dataframe with titles
df_title.columns = ['title']
```
Title: each title is a h2/href(URL) class, under h2 class. 'h2 class' under 'header class' under 'div class' under 'li class'

```{python}
### find date
span_blocks = soup1.find_all('span', attrs={'class': 'text-base-dark padding-right-105'})
span_blocks[0:5]
df_date = pd.DataFrame(span_blocks) 
df_date.columns = ['date']
#asked chat gpt 'how to i search for 'span' class with attribute xxx'
```
Date: is under span class


```{python}
### find category
li_blocks_dt = soup1.find_all('li', attrs={'class': 'display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1'})
li_blocks[0:5]
df_category = pd.DataFrame(li_blocks_dt)
df_category.columns = ['category']
```
Category: each title is a li class.'l1 class' under 'ul class' under 'div class' under 'header class'

```{python}
### find link
#use the list of titles from p1 and extract href
link_blocks = [link.get('href') for link in li_titles]
df_link = pd.DataFrame(link_blocks)
df_link.columns = ['link']
df_link['link'] = "https://oig.hhs.gov" + df_link['link']
```
Link: link=href class, under h2 class. 'h2 class' under 'header class' under 'div class' under 'li class'

```{python}
### combine dataframes
df_Q1 = pd.concat([df_title, df_date, df_category, df_link], axis=1)
df_Q1.head()
```


#### 2. (Partner 1) Crawling: Then for each enforcement action, click the link and collect the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office, Eastern District of Washington).



## (30 points) Step 2: Making the scraper dynamic

#### 1. Turning the scraper into a function: You will write a function that takes as input a month and a year, and then pulls and formats the enforcement actions like in Step 1 starting from that month+year to today.

#### a 

```{python}
#1) Check if the year if the year is 2013 or later if not, terminte function
#2) Create dictionary for params and use a while loop for pagnation setting for it to break when there's no next.
#3) Send Get request to get pages, dates, categories and links
#4) append new info to our dictionary
#5) wait one sec before going to the next page
```

#### b

```{python}
import time
from datetime import datetime

def enforcement_scrapper(start_month, start_year):
    if start_year < 2013:
        print("Please input a year >= 2013, as enforcement actions before 2013 are unavailable.")
        return

    base_url = 'https://oig.hhs.gov/fraud/enforcement'
    current_page = 1
    results = []

    while True:
        url = f"{base_url}/?page={current_page}"
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'lxml')
#titles links
        titles = []
        links = []
        for h2 in soup.find_all('h2'):
            a_tag = h2.find('a')
            if a_tag:
                titles.append(a_tag.get_text(strip=True))
                links.append("https://oig.hhs.gov" + a_tag['href'])

        #get dates
        dates = [span.get_text(strip=True) for span in soup.find_all('span', class_='text-base-dark padding-right-105')]

        #get categories
        categories = [
            li.get_text(strip=True) for li in soup.find_all('li', class_='display-inline-block usa-tag text-no-lowercase text-base-darkest bg-base-lightest margin-right-1')
        ]

        for title, date, category, link in zip(titles, dates, categories, links):
            results.append({'Title': title, 'Date': date, 'Category': category, 'Link': link})

        #check if page has dates earlier than when we're looking
        date_check = pd.to_datetime(dates, errors='coerce')
        if date_check.min() < pd.Timestamp(datetime(start_year, start_month, 1)):
            break
        
        current_page += 1


        
        time.sleep(1)

    
    scrapper = pd.DataFrame(results)
    scrapper['Date'] = pd.to_datetime(scrapper['Date'])
    start_date = datetime(start_year, start_month, 1)
    scrapper = scrapper[scrapper['Date'] >= start_date]
    return scrapper
twenty_three = enforcement_scrapper(1, 2023)  

print(twenty_three.shape[0])

```




```{python}
earliest_date = twenty_three['Date'].min()
print(f"Earliest Date: {earliest_date.date()}")

num_enforcements = len(twenty_three)
print(f"Number of Enforcement Actions: {num_enforcements}")
```


#### c

```{python}
twenty_one = enforcement_scrapper(1, 2021)

num_enforcements = len(twenty_one)
print(f"Number of Enforcement Actions: {num_enforcements}")

earliest_action = twenty_one.loc[twenty_one['Date'].idxmin()]
earliest_date = earliest_action['Date']
print(f"Earliest Date: {earliest_date.date()}")
print("Details of the Earliest Enforcement Action:")
print(earliest_action)
```

```{python}

#my code was taking me far far too long and so I used chat gpt to find how I can increase the speed of my crawler
import concurrent.futures
def scrape_link(link):
    try:
        response = requests.get(link)
        detail_soup = BeautifulSoup(response.content, 'lxml')
        agency_label = detail_soup.find('span', class_='padding-right-2 text-base', text='Agency:')
        if agency_label:
            # Get the actual text that follows the Agency: label
            agency_text = agency_label.next_sibling
            if agency_text:
                return agency_text.strip()
        return None
    except Exception as e:
        print(f"Error scraping {link}: {e}")
        return None

def main():
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        futures = {executor.submit(scrape_link, link): link for link in twenty_one['Link']}
        agencies = [future.result() for future in futures]

    twenty_one['Agency'] = agencies
    return twenty_one

results = main()
```


```{python}
print(f"Number of Enforcement Actions: {num_enforcements}")

earliest_action = twenty_one.loc[twenty_one['Date'].idxmin()]
earliest_date = earliest_action['Date']
print(f"Earliest Date: {earliest_date.date()}")
print("Details of the Earliest Enforcement Action:")
print(earliest_action)
```

## (15 points) Step 3: Plot data based on scraped data (using altair)

#### 1. (Partner 2) Plot a line chart that shows: the number of enforcement actions over time (aggregated to each month+year) overall since January 2021,

```{python}
import altair as alt

twenty_one['Month_Year'] = twenty_one['Date'].dt.to_period('M').dt.to_timestamp()
monthly_counts = twenty_one.groupby('Month_Year').size().reset_index(name='Count')

line_agg = alt.Chart(monthly_counts).mark_line(point = True).encode(
  x = alt.X('Month_Year', title = 'Month/Year'),
  y = alt.Y('Count:Q', title = 'Number of Actions'),
  tooltip=['Month_Year:T', 'Count:Q']
).properties( title = 'Number of Enforcement Actions Over Time (Aggregated by Month-Year)')

line_agg
```

#### 2. (Partner 1) Plot a line chart that shows: the number of enforcement actions split out by:  • “Criminal and Civil Actions” vs. “State Enforcement Agencies” • Five topics in the “Criminal and Civil Actions” category: “Health Care Fraud”, “Financial Fraud”, “Drug Enforcement”, “Bribery/Corruption”, and “Other”. 

## (15 points) Step 4: Create maps of enforcement activity For these questions, use this US Attorney District shapefile (link) and a Census state shapefile (link)

```{python}
os.chdir('/Users/willsigal/Desktop')
import geopandas as gpd
```

#### 1. (Partner 1) Map by state: Among actions taken by state-level agencies, clean the state names you collected and plot a choropleth of the number of enforcement actions for each state. Hint: look for “State of” in the agency info!

```{python}
state = gpd.read_file('cb_2018_us_state_500k/cb_2018_us_state_500k.shp')

state_geometries = state['geometry']
```

#### 2. (Partner 2) Map by district: Among actions taken by US Attorney District-level agencies, clean the district names so that you can merge them with the shapefile, and then plot a choropleth of the number of enforcement actions in each US Attorney District. Hint: look for “District” in the agency info.

```{python}
os.chdir('/Users/willsigal/Desktop/US Attorney Districts Shapefile simplified_20241108')
districts_gdf = gpd.read_file('geo_export_6b570657-b5c6-4bbc-b83a-415e4880d085.shp')

district_df = pd.DataFrame(districts_gdf)

```

```{python}
district_twentyone_df = twenty_one[twenty_one['Agency'].str.contains('District', na=False)].copy()

```

```{python}
district_df.head
```
```{python}
#removing the prefixes
district_twentyone_df['District'] = district_twentyone_df['Agency'].apply(lambda x: x.split(', ')[-1] if ', ' in x else x)
district_twentyone_df['District'] = district_twentyone_df['District'].str.strip()

district_twentyone_df['District'] = district_twentyone_df['District'].str.replace('U.S. Attorney\'s Office, ', '')

district_twentyone_df['District'].head
```

```{python}
enforcement_counts = district_twentyone_df['District'].value_counts().reset_index()
enforcement_counts.columns = ['District', 'Enforcement Actions']
```

```{python}
merged_districts_df = pd.merge(district_df, enforcement_counts, left_on='judicial_d', right_on='District', how='left')

merged_districts_df['Enforcement Actions'] = merged_districts_df['Enforcement Actions'].fillna(0)
merged_districts_df.columns
```

```{python}
gdf = gpd.GeoDataFrame(merged_districts_df, geometry='geometry')
print(gdf.crs)
gdf['Enforcement Actions'] = gdf['Enforcement Actions'].fillna(0)
```

```{python}
ig, ax = plt.subplots(figsize=(12, 8))  # Adjust figure size
gdf.plot(
    column='Enforcement Actions', 
    cmap='viridis', 
    legend=True, 
    ax=ax,
    edgecolor='black',  # Add borders for clarity
    linewidth=0.5)
ax.set_title('Enforcement Actions by Judicial District', fontsize=14)
#Removed Alska as it made the graph look horrible
ax.set_xlim([-130, -65])
ax.set_ylim([24, 50])
plt.show()
```



##  (10 points) Extra credit: Calculate the enforcement actions on a per-capita basis (Both partners can work together)

#### 1. Use the zip code shapefile from the previous problem set and merge it with zip code level population data. (Go to Census Data Portal, select “ZIP Code Tabulation Area”, check “All 5-digit ZIP Code Tabulation Areas within United States”, and under “P1 TOTAL POPULATION” select “2020: DEC Demographic and Housing Characteristics”. Download the csv.).

#### 2. Conduct a spatial join between zip code shapefile and the district shapefile, then aggregate to get population in each district.

#### 3. Mapthe ratio of enforcement actions in each US Attorney District. You can calculate the ratio by aggregating the number of enforcement actions since January 2021 per district, and dividing it with the population data.

